# -*- coding: utf-8 -*-
"""train_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BfMj0I7x_aEkqoDYnaTLpfZ9_TjfWvqg

# Подключение библиотек
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import *
from tensorflow.keras import utils
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
# %matplotlib inline
import csv
import random

"""# Функции для работы с CSV

Функция для перемешивания строк в CSV
"""

def shuffle_csv(filename, header=False, new_file=False):
    tmp_rows = []

    with open(filename, 'r') as csv_file:
        if header:
            next(csv_file) # skip header
        for row in csv_file:
            tmp_rows.append(row)

    random.shuffle(tmp_rows)

    if new_file:
        filename = filename[:-4] + '_shuffled.csv'
    with open(filename, 'w') as csv_file:
        for row in tmp_rows:
            csv_file.write(row)

"""Объединить два CSV файла в один"""

def merge_csv(filename1, filename2, newfilename, header=False):
    tmp_rows = []

    with open(filename1, 'r') as csv_file1:
        for row in csv_file1:
            tmp_rows.append(row)

    with open(filename2, 'r') as csv_file2:
        if header:
            next(csv_file2) # skip header
        for row in csv_file2:
            tmp_rows.append(row)

    with open(newfilename, 'w') as csv_file:
            for row in tmp_rows:
                csv_file.write(row)

    return print(len(tmp_rows), ' lines in new file')

def split_csv(filename, split_ratio):
    tmp_rows = []
    with open(filename, 'r') as mfile:
        for row in mfile:
            tmp_rows.append(row)

    pivot = int(len(tmp_rows) * split_ratio)

    rows_iter = enumerate(tmp_rows)

    with open(filename[:-4]+'_train_0,'+str(int(split_ratio*100))+'.csv', 'w') as part1_file:
        i = 0
        while i <= pivot:
            i, row = next(rows_iter)
            part1_file.write(row)
        
    with open(filename[:-4]+'_test_0,'+str(int(round(1-split_ratio, 2)*100))+'.csv', 'w') as part2_file:
        while i < len(tmp_rows)-1:
            i, row = next(rows_iter)
            part2_file.write(row)

    return print(f'First file {pivot} liens\nSecond file {len(tmp_rows)-pivot} lines')

"""Загрузка тренировочного и тестового датасета из CSV

*data_train, label_train, data_test, label_test*
"""

def load_data(train_file_name, test_file_name, verbose=0):
    x_train, y_train, x_test, y_test = [], [], [], []

    with open(train_file_name, 'r') as tfile:
        r = csv.reader(tfile)
        for row in r:
            y_train.append(row[0])
            x_train.append(row[1])

    with open(test_file_name, 'r') as efile:
        r = csv.reader(efile)
        for row in r:
            y_test.append(row[0])
            x_test.append(row[1])
    
    if verbose >= 1:
        print('Примеров в обучающей выбоке:' + str(len(x_train)))
        print('Примеров в тестовой выбоке:' + str(len(x_test)), end='\n\n')
    if verbose >= 2:
        print('Пример данных обучающей выборки:')
        print(x_train[5])
        print('Пример метки обучающей выборки:')
        print(y_train[5], end='\n\n')
        print('Пример данных тестовой выборки:')
        print(x_test[5])
        print('Пример метки обучающей выборки:')
        print(y_test[5],end='\n\n')
    if verbose == 3:
        print('Обучающая выборка впорядке: ' + str(len(x_train) == len(y_train)))
        print('Тестовая выборка впорядке: ' + str(len(x_test) == len(y_test)),end='\n\n')


    return x_train, y_train, x_test, y_test

def load_snippets_from_dataset(filename):
    labels, snippets = [], []
    with open(filename, 'r') as csvfile:
        r = csv.reader(csvfile)
        for row in r:
            labels.append(row[0])
            snippets.append(row[1])
    return labels, snippets

"""# Функции для работы со словарями

Класс Converter хранит словари токенов и меток

*   *Каждый токен и метка имеют уникальный индекс*
*   *Это позволяет преобразовывать данные*
"""

class Converter(object):
    def __init__(self, sequence):
        self.item2index_dict = {}
        self.index2item_dict = {}

        for i,item in enumerate(set(sequence)):
            self.item2index_dict[item] = i
            self.index2item_dict[i] = item

    def item_to_index(self, item):
        try:
            index = self.item2index_dict[item]
        except KeyError:
            index = self.item2index_dict['<MSD>']
        return index

    def index_to_item(self, i):
        return self.index2item_dict[i]

    def __len__(self):
        return len(self.item2index_dict)

    def to_file(self):
        with open('dictionary_'+ str(len(self)) + '.txt', 'w') as f:
            items = [item for item,index in self.item2index_dict.items()]
            f.write(','.join(items))

"""Проинициализировать конвертеры на основе данных"""

def build_converters(x_train, x_test, y_train, y_test, verbose=0):
    sequence = ','.join(x_train).split(',')
    sequence.extend(','.join(x_test).split(','))

    converter_data = Converter(sequence)

    sequence = list(y_test)
    sequence.extend(y_test)

    converter_labels = Converter(sequence)

    if verbose >= 1:
        print('Уникальных токенов: ' + str(len(converter_data)))
        print('Уникальных классов: ' + str(len(converter_labels)), end='\n\n')

    return converter_data, converter_labels

"""# Функции для конвертация данных в цифровой формат

Преобразование токенов и меток в цифровой формат


*   *С помощью Converter'a*
"""

def data_to_digital(sequence, converter, verbose=0):
    if len(sequence[0].split(',')) == 1:
        for i,item in enumerate(sequence):
            sequence[i] = converter.item_to_index(item)
    else:
        for i,row in enumerate(sequence):
            row = row.split(',')
            sequence[i] = []
            for item in row:
                sequence[i].append(converter.item_to_index(item))
    if verbose >= 1:
        print('Пример данных после data_to_digit: ')
        print(sequence[0])
        
    return sequence

"""Функция приводит данные к одной длине


*   Обрезает, если длинные
*   Дополняет нулями в начале, если короткие
"""

def to_same_length(sequence, maxlen, reverse=False, verbose=0):

    if reverse:
        for i,snippet in enumerate(sequence):
            sequence[i] = snippet[::-1]

    for i,row in enumerate(sequence):
        if len(row) > maxlen:
            sequence[i] = row[0:maxlen]

    sequence = pad_sequences(sequence, maxlen=maxlen)

    if verbose >= 1:
        print('Пример данных выборки после приведения к одной длине:')
        print(sequence[0],end='\n\n')

    return sequence

"""Функция для кодирования в one hot encoding"""

def vectorize_sequences(sequences, dimension, verbose=0):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.

    if verbose >= 1:
        print('Пример метки выборки:')
        print(results[0], end='\n\n')

    return results

"""# Конфигурация и загрузка данных"""

dic = 9000
thl = 300

# Названия файлов
train_file_name = f'ds_d{dic}_t{thl}_train_0,80.csv'
test_file_name = f'ds_d{dic}_t{thl}_test_0,20.csv'
# Максимальная длина последовательности
maxlen = 200
# Вывод информации о данных (1-выводить)
v = 0

# Загрузка данных
x_train, y_train, x_test, y_test = load_data(train_file_name, test_file_name, verbose=v*3)
# Инициализация словарей
cd, cc = build_converters(x_train, x_test, y_train, y_test, verbose=v)
# Конвертация токенов в цифровой формат
x_train = data_to_digital(x_train, cd, verbose=v)
y_train = data_to_digital(y_train, cc, verbose=v)
x_test = data_to_digital(x_test, cd, verbose=v)
y_test = data_to_digital(y_test, cc, verbose=v)
# Приведение примеров к одной длине
x_train = to_same_length(x_train, maxlen, reverse=False, verbose=v)
x_test = to_same_length(x_test, maxlen, reverse=False, verbose=v)
# Кодирование классов в One Hot Encoded
y_train = vectorize_sequences(y_train, len(cc), verbose=v)
y_test = vectorize_sequences(y_test, len(cc), verbose=v)

print(stopper)

delete_by_labels(['add', 'start'], x_train, y_train)
delete_by_labels(['add', 'start'], x_test, y_test)

def delete_by_labels(labels, x, y):
    labels = [cc.item_to_index(label) for label in labels]

    index = 0
    while index < len(y):
        if np.where(y == 1) in labels:
            del x[index]
            del y[index]
        else:
            index +=1

# сохранить словари в файл
cd.to_file()
cc.to_file()

"""# Выбор модели нейронной сети"""

# CNN-Model
model = Sequential()
model.add(Embedding(len(cd), 32, input_length=maxlen))
model.add(Conv1D(100, 10, activation='relu', padding='same'))
model.add(Conv1D(100, 10, activation='relu', padding='same'))
model.add(MaxPooling1D(3))
model.add(Conv1D(160, 10, activation='relu', padding='same'))
model.add(Conv1D(160, 10, activation='relu', padding='same'))
model.add(GlobalAveragePooling1D())
model.add(Dropout(0.5))
model.add(Dense(len(cc), activation='softmax'))

# LSTM
model = Sequential()
model.add(Embedding(len(cd), 32, input_length=maxlen))
model.add(LSTM(128, return_sequences=True, dropout=0.2))
model.add(LSTM(128, dropout=0.2))
model.add(Dense(len(cc), activation='softmax'))

model = Sequential()
model.add(Embedding(len(cd), 32, input_length=maxlen))
model.add(Conv1D(25, 10, activation='relu', padding='same'))
model.add(Conv1D(25, 10, activation='relu', padding='same'))
model.add(Conv1D(25, 10, activation='relu', padding='same'))
model.add(MaxPooling1D(2))
model.add(Conv1D(25, 10, activation='relu', padding='same'))
model.add(Conv1D(25, 10, activation='relu', padding='same'))
model.add(Conv1D(25, 10, activation='relu', padding='same'))
model.add(Conv1D(5, 10, activation='relu', padding='same'))
model.add(GlobalAveragePooling1D())
model.add(Dropout(0.5))
model.add(Dense(len(cc), activation='softmax'))

"""# Обучение нейронной сети"""

model.summary()

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
graph_history = {'accuracy': [], 'val_accuracy': []}

history = model.fit( x_train, y_train,
                            validation_data = (x_test, y_test),
                            epochs=25,
                            batch_size=100,
                            shuffle=True,
                            verbose=1)
graph_history['accuracy'].extend(history.history['accuracy'])
graph_history['val_accuracy'].extend(history.history['val_accuracy'])

for ie in range(0,20):
    history = model.fit( x_train, y_train,
                            validation_data = (x_test, y_test),
                            epochs=1,
                            batch_size=100,
                            shuffle=True,
                            verbose=1)
    epo = ie + 70
    acc = round(history.history['val_accuracy'][-1],4)
    model.save(f'gdrive/My Drive/models/CNN-mod-ds_d{dic}_t{thl}_len{maxlen}_e{epo}_p{acc}.h5')

"""# Сохранить/загрузить модель нейронной сети"""

from google.colab import drive
drive.mount('/content/gdrive')

"""Сохранение сети в файл"""

model.save(f'gdrive/My Drive/models/CNN-mod-ds_d{dic}_t{thl}_len{maxlen}_e6_p70.h5')

"""Загрузка модели из файла"""

model = load_model('LSTM.h5')

model.summary()

"""# Проверка качества обучения"""

plt.plot(graph_history['accuracy'],
         label='Доля верных ответов на обучающем наборе')
plt.plot(graph_history['val_accuracy'],
         label='Доля верных ответов на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

scores = model.evaluate(x_test, y_test, verbose=1)
print('Доля верных ответов на тестовых данных составляет:',round(scores[1]* 100, 4),'%')

"""# Функции для проверки"""

def predict_snippet(snippet):
    a = np.array(snippet)
    a = (np.expand_dims (a, 0))

    prediction = model.predict(a)[0]

    precent = np.max(prediction) * 100

    print('-------------------------------------')
    for name, value in get_top_predict_names(prediction):
        print(name + '\t\t' + str(round(value, 2)) + ' %')
    print('-------------------------------------', end='\n\n')

def get_top_predict_names(prediction, max=5):
    name_precent_list = []
    for i,value in enumerate(prediction):
        name_precent_list.append( (cc.index_to_item(i), value*100) )
    
    name_precent_list.sort(key=lambda x: x[1], reverse=True)

    return name_precent_list[:max]

# predict
def predict_list(indexes, predict_filter='', label_filter=''):
    for index in indexes:

        a = np.array(x_test[index])
        a = (np.expand_dims (a, 0))
        label = cc.index_to_item(np.argmax(y_test[index]))

        prediction = model.predict(a)[0]

        precent = np.max(prediction) * 100
        top5 = get_top_predict_names(prediction)

        if label in label_filter or label_filter == '':
            if top5[0][0] in predict_filter or predict_filter == '':
                print('Правильное название функции: ' + label)
                print('Предсказание : ')
                for name, value in top5:
                    print(name + '\t\t' + str(round(value, 2)) + ' %')
                print('-------------------------------------', end='\n\n')

"""# Проверка работы нейросети на реальных примерах

Проверка на тестовой выборке
"""

predict_list(range(5000, 5100), predict_filter='add')

"""Загрузка реальных примеров"""

filename_snippets = 'snip_d9000_t300.csv'
labels, snippets = load_snippets_from_dataset(filename_snippets)
snippets = data_to_digital(snippets, cd, verbose=0)
snippets = to_same_length(snippets, maxlen, reverse=False, verbose=0)
for i, label in enumerate(labels):
    print(i, ' - ', label)

for spin in snippets:
    predict_snippet(spin)

counter = 0
for y in y_train:
    if list(y).index(1) == cc.item_to_index('start'):
        counter += 1
counter

"""# Изучение Embedding'a

Получим матрицу векторов Embedding'a
"""

ebd_matrix = model.layers[0].get_weights()[0]

"""Визуализация плотных представлений всех токенов"""

plt.scatter(ebd_matrix[:,0], ebd_matrix[:,1])

"""Визуализация отдельных векторов"""

# По индексам
vec_indexes = [x for x in range(500,525)]
tokens_txt = []
for i in vec_indexes:
    tokens_txt.append(cd.index_to_item(i))

ebd_vectors = ebd_matrix[vec_indexes]

# По названиям
tokens_txt = ['stop', 'start', 'startup', 'test', 'test_simple',
              'close', 'IndexError', 'error', 'get', 'set', 'remove',
              'delete', 'old', 'new', 'KeyError', 'dict', 'res', 'result',
              'temp', 'tmp', 'text', 'txt', 'csv', 'json', 'config', 'cfg',
              'conf', 'sequence', 'seq', 'utf-8', 'ascii', 'time', 'datetime']
vec_indexes = []
for txt in tokens_txt:
    vec_indexes.append(cd.item_to_index(txt))

ebd_vectors = ebd_matrix[vec_indexes]

plt.scatter(ebd_vectors[:,0], ebd_vectors[:,1])
for i, txt in enumerate(tokens_txt):
    plt.annotate(txt, (ebd_vectors[i,0], ebd_vectors[i,1]))

for i in range(0, 1000):
    print(i, cd.index_to_item(i))