## 1. Постобработка датасета

​	Постобработка определяет некоторые параметры обучения, а точнее количество классов для классификации и размер словаря токенов. С этими параметрами можно экспериментировать, чтобы подобрать оптимальные значения.
​	Поэтому извлеченный набор данных из «Python ASTs» обрабатывается с разными параметрами несколько раз. В итоге получается несколько наборов данных для обучения с разными параметрами, которые указаны в таблице 1. Из параметров изменяется размер словаря токенов и порог вхождения функции в выборку. При увеличении словаря токенов, при отборе признаков, в последовательность будут попадать больше уникальных значений, а при уменьшении наоборот. При увеличении порога, количество классов будет сокращаться, а при уменьшении наоборот.

​	Общее количество функций в датасете равно 1274485. Уникальных имен функций (классов) - 535819; уникальных значений узлов (токенов) - 2437048.

Таблица 1 – **Сформированные наборы данных для обучения**

| №    | Порог вхождения токена в выборку | Порог вхождения функции в выборку | Количество функций для обучения | Количество токенов | Количество классов |
| ---- | :------------------------------: | :-------------------------------: | :-----------------------------: | :----------------: | :----------------: |
| 0    |              20000               |                400                |             155501              |        136         |        101         |
| 1    |              15000               |                300                |             183021              |        234         |        175         |
| 2    |              15000               |                200                |             200912              |        234         |        247         |
| 3    |               9000               |                300                |             182834              |        379         |        175         |
| 4    |               9000               |                200                |             200710              |        379         |        247         |
| 5    |               2000               |                300                |             182834              |        1498        |        175         |
| 6    |               2000               |                200                |             200912              |        1498        |        247         |

В итоге получилось 7 наборов данных с различными параметрами. Каждый набор пронумерован и далее обращение к наборам данных будет производиться по их номеру.

Notebook реализующий формирование датасетов: `functions_analysis.ipynb`

## 2. Эксперименты с моделями нейронных сетей

​	Модель задается большим количеством параметров, поэтому можно построить множество разных моделей. Сложно заранее определить какая архитектура модели будет справляться с задачей лучше. При построении модели следует учитывать тип решаемой задачи и опираться на известные практики. Добившись внушительных результатов можно начать экспериментировать для получения лучших результатов.

​	Для данной модели уже определены первый и последний слои. Первый слой является слоем Embedding, а последний слой полносвязный.
​	Все этапы подготовки данных для обучения производятся с помощью языка программирования Python в среде Google Colab. В подготовке данных используется стандартные библиотеки Python 3 и возможности Python 2.
​	Построение моделей производится тоже при помощи языка Python. Для этого используются две популярные библиотеки для работы с нейронными сетями: библиотека TensorFlow и библиотека Keras (является надстройкой над TensorFlow и еще несколькими библиотеками).
​	На предыдущих этапах обработки и подготовки данных к обучению, всегда обрабатывалась пара наборов. Один набор является обучающим, второй набор тестовым. Такое деление наборов необходимо для проверки того, как нейросеть справляется с примерами, которые она не наблюдала при обучении. Такой подход применяется потому, что нейросеть имеет свойство переобучаться и выдавать результат лучше на тех данных, на которых она учиться.
​	Точность работы модели определяется по тому, правильно ли она определяет метки примеров тестовой выборки, т.е. тех примеров, которых нейросеть не видела при обучении. **Точность** модели показывает, с какой вероятностью модель определяет правильную метку для примера данных. Именно такая точность является качественным показателем модели, чем больше эта точность, тем модель лучше.

​	На данном этапе будет производиться обучение моделей нейронных сетей на разных датасетах, подготовленных на предыдущем этапе. Замер их эффективности в зависимости от данных. И выбор лучшей модели. Обучение заканчивается, когда модель долгое время не улучшается или происходит переобучение.

​	Подробное описание моделей приведено в файле `model_architecture.md`.

### 2.1. Convolution Neural Network

​		Первая модель построена на основе сверточных слоев. Будем называть модели сочетанием аббревиатур используемых в них слоев. Первую модель назовем CNN (Convolution Neural Network). Помимо входного, выходного и сверточных слоев в модели присутствуют слои пулинга и слой прореживания. Структурная схема на рисунке 1.

![](D:/Projects/function-name-prediction/info/images/cnn_model.png)

​		Рисунок 1 – Структурная схема модели CNN

#### 2.1.1. Результаты обучения модели CNN

​	Результаты обучения модели отражены в таблице 2.

Таблица 2 – **Результаты обучения модели CNN**

|  №   | Набор данных, <br>(номер) | Длина последовательности | Количество эпох | Точность, % | Время обучения |
| :--: | :-----------------------: | :----------------------: | :-------------: | :---------: | :------------: |
|      |                           |                          |                 |             |                |
|      |                           |                          |                 |             |                |
|      |                           |                          |                 |             |                |
|      |                           |                          |                 |             |                |

> Наблюдения:
> а) Модель быстро учится. Для прохождения одной эпохи обучения при длине последовательности 300 требуется 30 секунд в среднем.
> б) При уменьшении порога вхождения функции в класс с 300 до 190 увеличивается количество распознаваемых функций на 92 названия. При обучении с меньшим порогом вхождения точность модели падает в среднем на 4.4%.
> в) При увеличении словаря и количества классов увеличивается размер обученной модели. Каждые 7000 новых токенов в словаре увеличивают размер модели на 2 МБ.
> г) Переобучение модели происходит быстро. Все графики обучения этой модели примерно выглядят как график на рисунке 18.
>
> Рисунок 18 – График обучения модели CNN
> д) После экспериментов 1-6 (таблица 2), установлено, что модель выдает лучшую точность на наборе данных 9000-300-111. Поэтому дальнейшие эксперименты производятся с этим набором.
> е) В экспериментах 7-9 (таблица 2) изменялась длина входной последовательности. Средняя длина последовательностей в наборе равна 50. По результатам эксперимента видно, что средней длины последовательности не достаточно. Так как при увеличении длины с 50 на 150 растет точность, но при дальнейшем увеличении рост точности незначительный. А скорость обучения при увеличении длины последовательности увеличивается (при длине 500 одна эпоха длится 56 секунд).

Таблица 3 – **Лучший результат модели CNN**

| Параметр                      | Значенеие |
| ----------------------------- | --------- |
| Набор данных                  |           |
| Количество классов            |           |
| Количество токенов            |           |
| Длинна последовательности     |           |
| Размер словаря, МБ            |           |
| Размер модели, МБ             |           |
| Выполнено эпох обучения       |           |
| Затрачено времени на обучение |           |
| Точность модели, %            |           |





### 2.2. Long Short Term Memory

​		Вторая модель построена на основе LSTM (Long Short Term Memory) слоев. LSTM являются модификацией рекуррентных слоев. Нейрон LSTM имеет более сложную структуру, которая лучше справляется с затуханием информации при обработке длинных последовательностей. Результаты обучения приведены в таблице 4. Структурная схема изображена на рисунке 2.

![](D:/Projects/function-name-prediction/info/images/lstm_model.png)

​		Рисунок 2 – Структурная схема модели CNN

#### 2.2.1. Результаты обучения модели LSTM

​	Результаты обучения модели отражены в таблице 3.

Таблица 3 – **Результаты обучения модели LSTM**

|  №   | Набор данных, <br>(номер) | Длина последовательности | Количество эпох | Точность, % | Время обучения |
| :--: | :-----------------------: | :----------------------: | :-------------: | :---------: | :------------: |
|      |                           |                          |                 |             |                |
|      |                           |                          |                 |             |                |
|      |                           |                          |                 |             |                |
|      |                           |                          |                 |             |                |

> Наблюдения:
>
> Модель намного меньше подвержена переобучению, чем CNN модель.
>
> а) Модель быстро учится. Для прохождения одной эпохи обучения при длине последовательности 300 требуется 30 секунд в среднем.
> б) При уменьшении порога вхождения функции в класс с 300 до 190 увеличивается количество распознаваемых функций на 92 названия. При обучении с меньшим порогом вхождения точность модели падает в среднем на 4.4%.
> в) При увеличении словаря и количества классов увеличивается размер обученной модели. Каждые 7000 новых токенов в словаре увеличивают размер модели на 2 МБ.
> г) Переобучение модели происходит быстро. Все графики обучения этой модели примерно выглядят как график на рисунке 18.
>
> Рисунок 18 – График обучения модели CNN
> д) После экспериментов 1-6 (таблица 2), установлено, что модель выдает лучшую точность на наборе данных 9000-300-111. Поэтому дальнейшие эксперименты производятся с этим набором.
> е) В экспериментах 7-9 (таблица 2) изменялась длина входной последовательности. Средняя длина последовательностей в наборе равна 50. По результатам эксперимента видно, что средней длины последовательности не достаточно. Так как при увеличении длины с 50 на 150 растет точность, но при дальнейшем увеличении рост точности незначительный. А скорость обучения при увеличении длины последовательности увеличивается (при длине 500 одна эпоха длится 56 секунд).

Таблица 3 – **Лучший результат модели CNN**

| Параметр                      | Значенеие |
| ----------------------------- | --------- |
| Набор данных                  |           |
| Количество классов            |           |
| Количество токенов            |           |
| Длинна последовательности     |           |
| Размер словаря, МБ            |           |
| Размер модели, МБ             |           |
| Выполнено эпох обучения       |           |
| Затрачено времени на обучение |           |
| Точность модели, %            |           |



## 3. Сравнение обученных моделей

​	Модели сравниваются по нескольким параметрам, но главные параметры - это точность модели и количество распознаваемых функций (классов). Выбирается самая точная модель и сравнивается со второй моделью, обученной на том же датасете, что и выбранная.

| Параметр           | CNN  | LSTM |
| ------------------ | ---- | ---- |
| Точность           |      |      |
| Время обучения     |      |      |
| Размер модели, МБ  |      |      |
| Размер словаря, МБ |      |      |
| Реальные примеры   |      |      |

> Точность: точность модели LSTM превосходит точность модели CNN на 2,66 %. Это может показаться как маленькая разница, но это не так. В обучении было использовано около 130 тысяч функций, и это мало. Если увеличить обучающий набор, то разница станет больше. На большом наборе LSTM будет заметно выигрывать.
> Время обучения: модель CNN учится быстрее. Так как вычисления в сверточных слоях можно распараллелить и выполнять на видеокарте, то сверточные слои тратят меньше времени на обучение. Слои же рекуррентные не поддаются распараллеливанию, потому что имеют обратную связь в нейронах, которая не известна заранее, а рассчитывается во время чтения последовательности. Модель LSTM достигает точность модели CNN после 28 минут обучения, что в 2 раза дольше CNN.
> Размер модели: размер модели LSTM меньше, чем CNN на 3.85 МБ. Размер зависит от: размера словаря токенов, так как матрица Embedding хранится в модели ; от количества внутренних параметров модели, которые
> 56
> являются внутренним состоянием. У LSTM модели таких параметров меньше.
> Реальные примеры: при классификации реальных примеров модель LSTM справилась лучше. Реальными примерами называются примеры не входящие в набор данных «Python ASTs». Примеры взяты из реальных открытых проектов, выложенных в интернет.
> Вывод: модель на основе рекуррентных слоев (LSTM) является более успешной, чем модель на основе сверточных слоев (CNN). Она выдает большую точность классификации равную 77.8 %, лучше справляется с реальными примерами, меньше переобучается и имеет меньший размер. Но LSTM учится долго. В 2 раза дольше, чтобы достигнуть уровень CNN. Поэтому, если необходимо более быстрое решение и можно пожертвовать точностью, то следует применить сверточные сети. А если точность важнее – использовать LSTM.











> Вывод по разделу
> В данном разделе решалась задача определения названия функции по абстрактному синтаксическому дереву ее тела. Задача решалась для языка программирования Python. Для решения этой задачи были построены несколько вариантов глубоких нейронных сетей, две лучшие были выбраны для экспериментирования. Точнее всего справилась модель нейронной сети, основанная на рекуррентных слоях. Эта модель правильно определяет название функции, которая не учувствовала в обучении, с вероятностью 77.8%. Но вычисления такой модели не поддаются распараллеливанию, поэтому она обучается дольше, чем модель, основанная на сверточных слоях, которая выдает близкий результат – 75.14%. Но ожидается, что на большем наборе данных точность рекуррентной модели будет заметно больше.
> Также, экспериментируя с набором данных, замечено, что для увеличения количества определяемых классов (названий функций) необходимо увеличить набор данных для обучения. В работе брался порог вхождения функции в класс 300. Если в наборе данных имелось не менее 300 примеров этой функции, то такая функция становилась классом.